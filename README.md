# Home_saless
ğŸ¡ Home Sales Analysis with PySpark
ğŸ“˜ Overview
In this challenge, we used Apache Spark and SparkSQL to explore and analyze a dataset of home sales. The main goal was to answer specific business questions about home prices by applying PySpark DataFrame transformations, caching, partitioning, and performance optimization.

ğŸ“‚ Project Setup
Create a new repository named Home_Sales.

Clone the repository to your local machine.

Rename the provided Home_Sales_starter_code.ipynb file to Home_Sales.ipynb.

Upload the completed notebook to your GitHub repo when finished.

ğŸ§ª Technologies Used
Python 3

Apache Spark / PySpark

SparkSQL

AWS S3 (for reading the dataset)

Jupyter Notebook

ğŸ“ Data Source
Filename: home_sales_revised.csv

Location: AWS S3 (as provided in challenge instructions)

ğŸ“Š Tasks Completed
Task	Description
âœ…	Loaded home sales data into a Spark DataFrame
âœ…	Created a temporary SQL table home_sales
âœ…	Ran SQL queries to answer business questions
âœ…	Cached and uncached data to compare performance
âœ…	Partitioned data by date_built and read Parquet
âœ…	Created a temporary table from Parquet data
âœ…	Measured query runtimes for performance comparison

ğŸ§  Questions Answered Using SparkSQL
Average price of four-bedroom homes per year

Average price of homes with 3 bed / 3 bath per year built

Average price of 3 bed / 3 bath / 2 floor homes â‰¥ 2000 sqft per year

Average price per "view" rating where avg price â‰¥ $350,000

Runtime was tracked and compared across:

Uncached

Cached

Partitioned Parquet format

ğŸš€ Performance Optimization
Step	Optimization
âœ…	Cached the home_sales table to speed up queries
âœ…	Measured query runtime with and without caching
âœ…	Partitioned data by date_built and read as Parquet
âœ…	Compared runtime of the same query on Parquet table
âœ…	Uncached the table and verified with Spark

âœ… Verification
All queries return results rounded to two decimal places

All runtimes were tracked using Python's time module

Cache and uncache status were checked using Spark's API

ğŸ’¾ How to Run
Ensure Spark is installed and running

Open Home_Sales.ipynb in Jupyter

Run each cell in order

Modify the AWS S3 path if needed to access the CSV

ğŸ“Œ Submission
Final file: Home_Sales.ipynb

Submitted via GitHub to the Home_Sales repository
